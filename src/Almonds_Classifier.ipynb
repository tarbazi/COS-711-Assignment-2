{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Length (major axis)</th>\n",
       "      <th>Width (minor axis)</th>\n",
       "      <th>Thickness (depth)</th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Roundness</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>Aspect Ratio</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Convex hull(convex area)</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>227.940628</td>\n",
       "      <td>127.759132</td>\n",
       "      <td>22619.0</td>\n",
       "      <td>643.813269</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.973384</td>\n",
       "      <td>1.458265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.681193</td>\n",
       "      <td>23237.5</td>\n",
       "      <td>MAMRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>234.188126</td>\n",
       "      <td>128.199509</td>\n",
       "      <td>23038.0</td>\n",
       "      <td>680.984841</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.957304</td>\n",
       "      <td>1.601844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.656353</td>\n",
       "      <td>24065.5</td>\n",
       "      <td>MAMRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229.418610</td>\n",
       "      <td>125.796547</td>\n",
       "      <td>22386.5</td>\n",
       "      <td>646.943212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.967270</td>\n",
       "      <td>1.487772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.683620</td>\n",
       "      <td>23144.0</td>\n",
       "      <td>MAMRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>232.763153</td>\n",
       "      <td>125.918808</td>\n",
       "      <td>22578.5</td>\n",
       "      <td>661.227483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.965512</td>\n",
       "      <td>1.540979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.685360</td>\n",
       "      <td>23385.0</td>\n",
       "      <td>MAMRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>230.150742</td>\n",
       "      <td>107.253448</td>\n",
       "      <td>19068.0</td>\n",
       "      <td>624.842706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.951450</td>\n",
       "      <td>1.629395</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.714800</td>\n",
       "      <td>20041.0</td>\n",
       "      <td>MAMRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Length (major axis)  Width (minor axis)  Thickness (depth)  \\\n",
       "0           0                  NaN          227.940628         127.759132   \n",
       "1           1                  NaN          234.188126         128.199509   \n",
       "2           2                  NaN          229.418610         125.796547   \n",
       "3           3                  NaN          232.763153         125.918808   \n",
       "4           4                  NaN          230.150742         107.253448   \n",
       "\n",
       "      Area   Perimeter  Roundness  Solidity  Compactness  Aspect Ratio  \\\n",
       "0  22619.0  643.813269        NaN  0.973384     1.458265           NaN   \n",
       "1  23038.0  680.984841        NaN  0.957304     1.601844           NaN   \n",
       "2  22386.5  646.943212        NaN  0.967270     1.487772           NaN   \n",
       "3  22578.5  661.227483        NaN  0.965512     1.540979           NaN   \n",
       "4  19068.0  624.842706        NaN  0.951450     1.629395           NaN   \n",
       "\n",
       "   Eccentricity    Extent  Convex hull(convex area)   Type  \n",
       "0           NaN  0.681193                   23237.5  MAMRA  \n",
       "1           NaN  0.656353                   24065.5  MAMRA  \n",
       "2           NaN  0.683620                   23144.0  MAMRA  \n",
       "3           NaN  0.685360                   23385.0  MAMRA  \n",
       "4           NaN  0.714800                   20041.0  MAMRA  "
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bazi Mathangeni\n",
    "#22/09/2024\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "myRawData = pd.read_csv(\"../input_data/Almond.csv\")\n",
    "\n",
    "myRawData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Length (major axis)</th>\n",
       "      <th>Width (minor axis)</th>\n",
       "      <th>Thickness (depth)</th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Roundness</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>Aspect Ratio</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Convex hull(convex area)</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>315.512085</td>\n",
       "      <td>169.067093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38551.5</td>\n",
       "      <td>831.085347</td>\n",
       "      <td>0.493082</td>\n",
       "      <td>0.983494</td>\n",
       "      <td>1.425739</td>\n",
       "      <td>1.866195</td>\n",
       "      <td>0.844313</td>\n",
       "      <td>0.777060</td>\n",
       "      <td>39198.5</td>\n",
       "      <td>MAMRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>318.522736</td>\n",
       "      <td>168.491837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38867.5</td>\n",
       "      <td>829.712764</td>\n",
       "      <td>0.487771</td>\n",
       "      <td>0.980549</td>\n",
       "      <td>1.409480</td>\n",
       "      <td>1.890434</td>\n",
       "      <td>0.848635</td>\n",
       "      <td>0.773575</td>\n",
       "      <td>39638.5</td>\n",
       "      <td>MAMRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>320.810455</td>\n",
       "      <td>168.278076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39255.5</td>\n",
       "      <td>834.884337</td>\n",
       "      <td>0.485639</td>\n",
       "      <td>0.982653</td>\n",
       "      <td>1.413000</td>\n",
       "      <td>1.906430</td>\n",
       "      <td>0.851385</td>\n",
       "      <td>0.783778</td>\n",
       "      <td>39948.5</td>\n",
       "      <td>MAMRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>317.151794</td>\n",
       "      <td>167.854141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39163.0</td>\n",
       "      <td>838.641696</td>\n",
       "      <td>0.495737</td>\n",
       "      <td>0.982255</td>\n",
       "      <td>1.429115</td>\n",
       "      <td>1.889449</td>\n",
       "      <td>0.848463</td>\n",
       "      <td>0.772141</td>\n",
       "      <td>39870.5</td>\n",
       "      <td>MAMRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>397.007843</td>\n",
       "      <td>188.688644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55550.0</td>\n",
       "      <td>1025.837653</td>\n",
       "      <td>0.448741</td>\n",
       "      <td>0.974989</td>\n",
       "      <td>1.507521</td>\n",
       "      <td>2.104037</td>\n",
       "      <td>0.879836</td>\n",
       "      <td>0.710458</td>\n",
       "      <td>56975.0</td>\n",
       "      <td>MAMRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Length (major axis)  Width (minor axis)  Thickness (depth)  \\\n",
       "16          16           315.512085          169.067093                NaN   \n",
       "17          17           318.522736          168.491837                NaN   \n",
       "18          18           320.810455          168.278076                NaN   \n",
       "19          19           317.151794          167.854141                NaN   \n",
       "20          20           397.007843          188.688644                NaN   \n",
       "\n",
       "       Area    Perimeter  Roundness  Solidity  Compactness  Aspect Ratio  \\\n",
       "16  38551.5   831.085347   0.493082  0.983494     1.425739      1.866195   \n",
       "17  38867.5   829.712764   0.487771  0.980549     1.409480      1.890434   \n",
       "18  39255.5   834.884337   0.485639  0.982653     1.413000      1.906430   \n",
       "19  39163.0   838.641696   0.495737  0.982255     1.429115      1.889449   \n",
       "20  55550.0  1025.837653   0.448741  0.974989     1.507521      2.104037   \n",
       "\n",
       "    Eccentricity    Extent  Convex hull(convex area)   Type  \n",
       "16      0.844313  0.777060                   39198.5  MAMRA  \n",
       "17      0.848635  0.773575                   39638.5  MAMRA  \n",
       "18      0.851385  0.783778                   39948.5  MAMRA  \n",
       "19      0.848463  0.772141                   39870.5  MAMRA  \n",
       "20      0.879836  0.710458                   56975.0  MAMRA  "
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myCleanData = myRawData.dropna(subset = [\"Area\", \"Perimeter\", \"Roundness\", \"Solidity\", \"Compactness\", \"Aspect Ratio\", \"Eccentricity\", \"Extent\", \"Convex hull(convex area)\", \"Type\"])\n",
    "\n",
    "myCleanData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "MAMRA = myCleanData[myCleanData[\"Type\"] == \"MAMRA\"]\n",
    "MAMRA = MAMRA.to_numpy()\n",
    "\n",
    "np.random.shuffle(MAMRA)\n",
    "\n",
    "MAMRA_TRAINING = MAMRA[ : math.floor(0.8*len(MAMRA))]\n",
    "MAMRA_TEST = MAMRA[ math.floor(0.8*len(MAMRA)) : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "SANORA = myCleanData[myCleanData[\"Type\"] == \"SANORA\"]\n",
    "SANORA = SANORA.to_numpy()\n",
    "\n",
    "np.random.shuffle(SANORA)\n",
    "\n",
    "SANORA_TRAINING = SANORA[ : math.floor(0.8*len(SANORA))]\n",
    "SANORA_TEST = SANORA[ math.floor(0.8*len(MAMRA)) : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGULAR = myCleanData[myCleanData[\"Type\"] == \"REGULAR\"]\n",
    "\n",
    "REGULAR = REGULAR.to_numpy()\n",
    "\n",
    "np.random.shuffle(REGULAR)\n",
    "\n",
    "REGULAR_TRAINING = REGULAR[ : math.floor(0.8*len(REGULAR))]\n",
    "REGULAR_TEST = REGULAR[ math.floor(0.8*len(MAMRA)) : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAMRA_SANORA_TRAINING = np.vstack((MAMRA_TRAINING, SANORA_TRAINING))\n",
    "FINAL_TRAINING_DATA = np.vstack((MAMRA_SANORA_TRAINING, REGULAR_TRAINING))\n",
    "\n",
    "np.random.shuffle(FINAL_TRAINING_DATA)\n",
    "\n",
    "MAMRA_SANORA_TEST = np.vstack((MAMRA_TEST, SANORA_TEST))\n",
    "FINAL_TEST_DATA = np.vstack((MAMRA_SANORA_TEST, REGULAR_TEST))\n",
    "\n",
    "np.random.shuffle(FINAL_TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorVariables = FINAL_TRAINING_DATA[ : , :13]\n",
    "predictorVariables = np.float64(predictorVariables)\n",
    "\n",
    "dependentVariable = FINAL_TRAINING_DATA[ : ,13: ]\n",
    "\n",
    "dependentVariable[dependentVariable == \"MAMRA\"] = np.float64(0)\n",
    "dependentVariable[dependentVariable == \"SANORA\"] = np.float64(1)\n",
    "dependentVariable[dependentVariable == \"REGULAR\"] = np.float64(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredictorVariables= FINAL_TEST_DATA[ : , :13]\n",
    "testPredictorVariables = np.float64(testPredictorVariables)\n",
    "\n",
    "testDependentVariable = FINAL_TEST_DATA[ : ,13: ]\n",
    "\n",
    "testDependentVariable[testDependentVariable == \"MAMRA\"] = np.float64(0)\n",
    "testDependentVariable[testDependentVariable == \"SANORA\"] = np.float64(1)\n",
    "testDependentVariable[testDependentVariable == \"REGULAR\"] = np.float64(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I standard my data to have a mean of 0 and variance of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeAndCleanData(predictorVariables):\n",
    "\n",
    "    pV_normalized = (predictorVariables - predictorVariables.mean(axis = 0))/(predictorVariables.std(axis = 0))\n",
    "    pV_normalized[np.isnan(pV_normalized)] = np.float64(0)\n",
    "\n",
    "    return pV_normalized\n",
    "\n",
    "pV_normalized = normalizeAndCleanData(predictorVariables)\n",
    "testPV_normalized = normalizeAndCleanData(testPredictorVariables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function to compute the sigmoid activation value of inputs incoming into a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \n",
    "    return (1/(1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid derivative function to compute gradient values during backpropation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidDerivative(x):\n",
    "    \n",
    "    return (x*(1-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used (Error Sum Of Squares)/2 as my Error/Loss function, hence the function below compute the derivative of the outermost term of my error function. The product of this function, the above mentioned sigmoid function derivative and the values at the neurons of the layer preceding the output layer forms the overall derivative of the Loss Function with respect to Weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outerTermDerivative(targetVal, estimatedVal):\n",
    "    \n",
    "    return (targetVal - estimatedVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to tranform the values of the models dependent variables from an array of values (1 by n) values of (0 = MAMRA, 1 = SANORA, 2 = REGULAR) to an array (3 by n) values with ([1, 0, 0] = MAMRA, [0, 1, 0] = SANORA and [0, 0, 1] = REGULAR).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(yVals):\n",
    "\n",
    "    newArr = np.zeros((yVals.shape[0], 3))\n",
    "\n",
    "    for i in range(len(yVals)):\n",
    "        \n",
    "        if yVals[i] == 0:\n",
    "            newArr[i][0] += 1\n",
    "        elif yVals[i] == 1:\n",
    "            newArr[i][1] += 1\n",
    "        else:\n",
    "            newArr[i,2] += 1\n",
    "\n",
    "    return newArr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model is designed such that the brightness of the output neurons with respect to an input in predicting the out is expressed as a floating point values, with the most likely correct output yielding the highest values. This function supresses the 2 non maximum values to 0 and increases the maximum value to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(yVals):\n",
    "    \n",
    "    for i in range(len(yVals)):\n",
    "        \n",
    "        if (np.max(yVals[i]) == yVals[i][0]):\n",
    "            yVals[i][0] = 1\n",
    "            yVals[i][1] = 0\n",
    "            yVals[i][2] = 0\n",
    "        elif (np.max(yVals[i]) == yVals[i][1]):\n",
    "            yVals[i][0] = 0\n",
    "            yVals[i][1] = 1\n",
    "            yVals[i][2] = 0\n",
    "        elif (np.max(yVals[i]) == yVals[i][2]):\n",
    "            yVals[i][0] = 0\n",
    "            yVals[i][1] = 0\n",
    "            yVals[i][2] = 1\n",
    "\n",
    "    return yVals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my actual Forward Feed and Backpropagation model. It takes in the input values (xVals), target values as (yVals), input layer and hidden layer weights (w1), hidden layer and output layer weights (w2), learning rate value (learningRate) and number of epochs (numEs).\n",
    "\n",
    "I have commented what each of lines of code does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelNetwork1(xVals, yVals, w1, w2, learningRate, numEs):\n",
    "\n",
    "    for i in range(numEs):\n",
    "\n",
    "        hiddenLayerParemeters = xVals.dot(w1) #multiplies and sums input layer values with their respective weights in preparation to feed the hidden layer as inputs\n",
    "        hiddenLayerVals = sigmoid(hiddenLayerParemeters) #applies the sigmoid activation function on the incoming paremeters\n",
    "\n",
    "        outputLayerParameter = hiddenLayerVals.dot(w2) #multiplies the hidden layer values their respective weights in preparation to feed the output layer as inputs\n",
    "        outputVal = sigmoid(outputLayerParameter) #applied the sigmoid activation function on the incoming parameters to yield final output layer values\n",
    "\n",
    "        derivativeOfOuterTerm = outerTermDerivative(yVals, outputVal) #computes the first term of the loss function value\n",
    "        derivativeOfInnerTerm = sigmoidDerivative(outputVal) #computes the second term of the loss function value\n",
    "\n",
    "        hidden_OutputLayerErrorTerm = derivativeOfOuterTerm * derivativeOfInnerTerm #this is the product of the 2 terms\n",
    "\n",
    "        lossFunctionDerivative = hiddenLayerVals.T.dot(hidden_OutputLayerErrorTerm.astype(np.float64)) #this is the product of all 3 terms, yielding the actual loss function derivative value with respect to hidden layer and output layer weights\n",
    "        w2 += lossFunctionDerivative*learningRate #multiplies the learning rate against the loss function derivative then updates the hidden layer - output layer weights\n",
    "        \n",
    "        input_HiddenLayerErrorTerm = hidden_OutputLayerErrorTerm.dot(w2.T) * sigmoidDerivative(hiddenLayerVals) #computes the loss function derivative with respect to input layer - hidden layer weights\n",
    "\n",
    "        w1 += xVals.T.dot(input_HiddenLayerErrorTerm.astype(np.float64)) * learningRate #multiples the learning rate against the loss function derivative then updates the input layer - hidden layer weights\n",
    "\n",
    "    return (w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model1(xVals, yVals, w1, w2, learningRate):\n",
    "\n",
    "    hiddenLayerParemeters = xVals.dot(w1) #multiplies and sums input layer values with their respective weights in preparation to feed the hidden layer as inputs\n",
    "    hiddenLayerVals = sigmoid(hiddenLayerParemeters) #applies the sigmoid activation function on the incoming paremeters\n",
    "\n",
    "    outputLayerParameter = hiddenLayerVals.dot(w2) #multiplies the hidden layer values their respective weights in preparation to feed the output layer as inputs\n",
    "    outputVal = sigmoid(outputLayerParameter) #applied the sigmoid activation function on the incoming parameters to yield final output layer values\n",
    "\n",
    "    derivativeOfOuterTerm = outerTermDerivative(yVals, outputVal) #computes the first term of the loss function value\n",
    "    derivativeOfInnerTerm = sigmoidDerivative(outputVal) #computes the second term of the loss function value\n",
    "\n",
    "    hidden_OutputLayerErrorTerm = derivativeOfOuterTerm * derivativeOfInnerTerm #this is the product of the 2 terms\n",
    "\n",
    "    lossFunctionDerivative = hiddenLayerVals.T.dot(hidden_OutputLayerErrorTerm.astype(np.float64)) #this is the product of all 3 terms, yielding the actual loss function derivative value with respect to hidden layer and output layer weights\n",
    "    w2 += lossFunctionDerivative*learningRate #multiplies the learning rate against the loss function derivative then updates the hidden layer - output layer weights\n",
    "    \n",
    "    input_HiddenLayerErrorTerm = hidden_OutputLayerErrorTerm.dot(w2.T) * sigmoidDerivative(hiddenLayerVals) #computes the loss function derivative with respect to input layer - hidden layer weights\n",
    "\n",
    "    w1 += xVals.T.dot(input_HiddenLayerErrorTerm.astype(np.float64)) * learningRate #multiples the learning rate against the loss function derivative then updates the input layer - hidden layer weights\n",
    "\n",
    "    return xVals.T.dot(input_HiddenLayerErrorTerm.astype(np.float64)), lossFunctionDerivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model2(xVals, yVals, w1, w2, learningRate1, learningRate2):\n",
    "\n",
    "    hiddenLayerParemeters = xVals.dot(w1) #multiplies and sums input layer values with their respective weights in preparation to feed the hidden layer as inputs\n",
    "    hiddenLayerVals = sigmoid(hiddenLayerParemeters) #applies the sigmoid activation function on the incoming paremeters\n",
    "\n",
    "    outputLayerParameter = hiddenLayerVals.dot(w2) #multiplies the hidden layer values their respective weights in preparation to feed the output layer as inputs\n",
    "    outputVal = sigmoid(outputLayerParameter) #applied the sigmoid activation function on the incoming parameters to yield final output layer values\n",
    "\n",
    "    derivativeOfOuterTerm = outerTermDerivative(yVals, outputVal) #computes the first term of the loss function value\n",
    "    derivativeOfInnerTerm = sigmoidDerivative(outputVal) #computes the second term of the loss function value\n",
    "\n",
    "    hidden_OutputLayerErrorTerm = derivativeOfOuterTerm * derivativeOfInnerTerm #this is the product of the 2 terms\n",
    "\n",
    "    lossFunctionDerivative = hiddenLayerVals.T.dot(hidden_OutputLayerErrorTerm.astype(np.float64)) #this is the product of all 3 terms, yielding the actual loss function derivative value with respect to hidden layer and output layer weights\n",
    "    w2 += lossFunctionDerivative*learningRate2 #multiplies the learning rate against the loss function derivative then updates the hidden layer - output layer weights\n",
    "    \n",
    "    input_HiddenLayerErrorTerm = hidden_OutputLayerErrorTerm.dot(w2.T) * sigmoidDerivative(hiddenLayerVals) #computes the loss function derivative with respect to input layer - hidden layer weights\n",
    "\n",
    "    w1 += xVals.T.dot(input_HiddenLayerErrorTerm.astype(np.float64)) * learningRate1 #multiples the learning rate against the loss function derivative then updates the input layer - hidden layer weights\n",
    "\n",
    "    return xVals.T.dot(input_HiddenLayerErrorTerm.astype(np.float64)), lossFunctionDerivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLearningRateIndex(prev_m1, prev_m2, m1, m2, i1, i2):\n",
    "\n",
    "    commonDir1 = 0\n",
    "    total1 = 0\n",
    "\n",
    "    for i in range(prev_m1.shape[0]):\n",
    "        for j in range(prev_m1.shape[1]):\n",
    "            total1 += 1\n",
    "            if (prev_m1[i, j]*m1[i, j] > 0):\n",
    "                commonDir1 += 1\n",
    "\n",
    "    commonDir2 = 0\n",
    "    total2 = 0\n",
    "    \n",
    "    for i in range(prev_m2.shape[0]):\n",
    "        for j in range(prev_m2.shape[1]):\n",
    "            total2 += 1\n",
    "            if (prev_m2[i, j]*m2[i, j]  > 0):\n",
    "                commonDir2 += 1\n",
    "\n",
    "    if ((commonDir1/total1) > 0.5):\n",
    "        if i1 < 4:\n",
    "            i1 += 1\n",
    "    \n",
    "    elif ((commonDir1/total1) <= 0.5):\n",
    "        i1 = 0\n",
    "\n",
    "    if ((commonDir2/total2) > 0.5):\n",
    "        if i2 < 4:\n",
    "            i2 += 1\n",
    "    \n",
    "    elif ((commonDir2/total2) <= 0.5):\n",
    "        i2 = 0\n",
    "\n",
    "    return i1, i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(xVals, yVals, weightsInput_Hidden, weightsHidden_Output):\n",
    "\n",
    "    transformedDV = transform(yVals)\n",
    "\n",
    "    matches = sigmoid(sigmoid(xVals.dot(weightsInput_Hidden)).dot(weightsHidden_Output))\n",
    "\n",
    "    a = 0\n",
    "    hits = 0\n",
    "    classifyVals = classify(matches)\n",
    "\n",
    "    for val in classifyVals:\n",
    "        if ((val[0] == transformedDV[a][0]) & (val[1] == transformedDV[a][1]) & (val[2] == transformedDV[a][2])):\n",
    "            hits += 1\n",
    "        a += 1\n",
    "\n",
    "    return ((hits/a)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial 1 (13, 20) (20, 3)\n",
      "Initial 2 (13, 20) (20, 3)\n",
      "Model 1 23.076923076923077\n",
      "Model 2 23.076923076923077\n",
      "Model 3 23.076923076923077\n",
      "Model 1 23.076923076923077\n",
      "Model 2 23.776223776223777\n",
      "Model 3 23.076923076923077\n",
      "Model 1 40.55944055944056\n",
      "Model 2 42.65734265734265\n",
      "Model 3 51.74825174825175\n",
      "Model 1 58.74125874125874\n",
      "Model 2 54.54545454545454\n",
      "Model 3 55.94405594405595\n",
      "Model 1 60.13986013986013\n",
      "Model 2 55.94405594405595\n",
      "Model 3 57.34265734265735\n",
      "Model 1 62.23776223776224\n",
      "Model 2 60.83916083916085\n",
      "Model 3 58.04195804195804\n",
      "Model 1 62.93706293706294\n",
      "Model 2 62.93706293706294\n",
      "Model 3 58.04195804195804\n",
      "Model 1 63.63636363636363\n",
      "Model 2 60.13986013986013\n",
      "Model 3 58.04195804195804\n",
      "Model 1 61.53846153846154\n",
      "Model 2 57.34265734265735\n",
      "Model 3 58.04195804195804\n",
      "Model 1 62.23776223776224\n",
      "Model 2 59.44055944055944\n",
      "Model 3 58.04195804195804\n",
      "Model 1 63.63636363636363\n",
      "Model 2 51.74825174825175\n",
      "Model 3 58.74125874125874\n",
      "Model 1 63.63636363636363\n",
      "Model 2 51.74825174825175\n",
      "Model 3 55.94405594405595\n",
      "Model 1 60.83916083916085\n",
      "Model 2 55.24475524475524\n",
      "Model 3 55.94405594405595\n",
      "Model 1 60.13986013986013\n",
      "Model 2 55.94405594405595\n",
      "Model 3 55.94405594405595\n",
      "Model 1 60.13986013986013\n",
      "Model 2 57.34265734265735\n",
      "Model 3 55.94405594405595\n",
      "Model 1 60.83916083916085\n",
      "Model 2 58.04195804195804\n",
      "Model 3 56.64335664335665\n",
      "Model 1 61.53846153846154\n",
      "Model 2 58.04195804195804\n",
      "Model 3 56.64335664335665\n",
      "Model 1 59.44055944055944\n",
      "Model 2 63.63636363636363\n",
      "Model 3 55.94405594405595\n",
      "Model 1 64.33566433566433\n",
      "Model 2 65.73426573426573\n",
      "Model 3 56.64335664335665\n",
      "Model 1 60.83916083916085\n",
      "Model 2 69.23076923076923\n",
      "Model 3 55.94405594405595\n",
      "Model 1 60.83916083916085\n",
      "Model 2 72.72727272727273\n",
      "Model 3 56.64335664335665\n",
      "Model 1 60.83916083916085\n",
      "Model 2 62.23776223776224\n",
      "Model 3 55.94405594405595\n",
      "Model 1 62.93706293706294\n",
      "Model 2 57.34265734265735\n",
      "Model 3 54.54545454545454\n",
      "Model 1 57.34265734265735\n",
      "Model 2 57.34265734265735\n",
      "Model 3 55.24475524475524\n",
      "Model 1 60.13986013986013\n",
      "Model 2 57.34265734265735\n",
      "Model 3 55.24475524475524\n",
      "Model 1 63.63636363636363\n",
      "Model 2 58.04195804195804\n",
      "Model 3 54.54545454545454\n",
      "Model 1 56.64335664335665\n",
      "Model 2 55.94405594405595\n",
      "Model 3 55.94405594405595\n",
      "Model 1 60.13986013986013\n",
      "Model 2 59.44055944055944\n",
      "Model 3 55.94405594405595\n",
      "Model 1 67.13286713286713\n",
      "Model 2 56.64335664335665\n",
      "Model 3 55.94405594405595\n",
      "Model 1 58.04195804195804\n",
      "Model 2 58.74125874125874\n",
      "Model 3 55.94405594405595\n",
      "Model 1 63.63636363636363\n",
      "Model 2 60.83916083916085\n",
      "Model 3 55.94405594405595\n",
      "Model 1 65.73426573426573\n",
      "Model 2 60.13986013986013\n",
      "Model 3 56.64335664335665\n",
      "Model 1 57.34265734265735\n",
      "Model 2 59.44055944055944\n",
      "Model 3 55.24475524475524\n",
      "Model 1 63.63636363636363\n",
      "Model 2 60.13986013986013\n",
      "Model 3 55.94405594405595\n",
      "Model 1 65.03496503496503\n",
      "Model 2 60.83916083916085\n",
      "Model 3 58.74125874125874\n",
      "Model 1 66.43356643356644\n",
      "Model 2 60.83916083916085\n",
      "Model 3 56.64335664335665\n",
      "Model 1 65.03496503496503\n",
      "Model 2 61.53846153846154\n",
      "Model 3 56.64335664335665\n",
      "Model 1 64.33566433566433\n",
      "Model 2 62.23776223776224\n",
      "Model 3 56.64335664335665\n",
      "Model 1 64.33566433566433\n",
      "Model 2 63.63636363636363\n",
      "Model 3 56.64335664335665\n",
      "Model 1 65.03496503496503\n",
      "Model 2 63.63636363636363\n",
      "Model 3 57.34265734265735\n",
      "Model 1 62.23776223776224\n",
      "Model 2 62.23776223776224\n",
      "Model 3 56.64335664335665\n",
      "Model 1 67.83216783216784\n",
      "Model 2 62.23776223776224\n",
      "Model 3 58.04195804195804\n",
      "Model 1 58.74125874125874\n",
      "Model 2 64.33566433566433\n",
      "Model 3 56.64335664335665\n",
      "Model 1 67.13286713286713\n",
      "Model 2 65.03496503496503\n",
      "Model 3 56.64335664335665\n",
      "Model 1 53.84615384615385\n",
      "Model 2 66.43356643356644\n",
      "Model 3 57.34265734265735\n",
      "Model 1 65.73426573426573\n",
      "Model 2 63.63636363636363\n",
      "Model 3 55.24475524475524\n",
      "Model 1 54.54545454545454\n",
      "Model 2 65.03496503496503\n",
      "Model 3 58.04195804195804\n",
      "Model 1 65.73426573426573\n",
      "Model 2 66.43356643356644\n",
      "Model 3 58.04195804195804\n",
      "Model 1 68.53146853146853\n",
      "Model 2 66.43356643356644\n",
      "Model 3 58.74125874125874\n",
      "Model 1 69.23076923076923\n",
      "Model 2 65.73426573426573\n",
      "Model 3 77.62237762237763\n",
      "Model 1 69.23076923076923\n",
      "Model 2 65.73426573426573\n",
      "Model 3 67.13286713286713\n",
      "Model 1 69.23076923076923\n",
      "Model 2 67.83216783216784\n",
      "Model 3 68.53146853146853\n",
      "Model 1 69.23076923076923\n",
      "Model 2 72.02797202797203\n",
      "Model 3 64.33566433566433\n",
      "Model 1 70.62937062937063\n",
      "Model 2 73.42657342657343\n",
      "Model 3 66.43356643356644\n",
      "Model 1 70.62937062937063\n",
      "Model 2 70.62937062937063\n",
      "Model 3 62.23776223776224\n",
      "Model 1 72.02797202797203\n",
      "Model 2 73.42657342657343\n",
      "Model 3 65.73426573426573\n",
      "Model 1 72.72727272727273\n",
      "Model 2 73.42657342657343\n",
      "Model 3 64.33566433566433\n",
      "Model 1 71.32867132867133\n",
      "Model 2 73.42657342657343\n",
      "Model 3 67.83216783216784\n",
      "Model 1 78.32167832167832\n",
      "Model 2 74.12587412587412\n",
      "Model 3 66.43356643356644\n",
      "Model 1 75.52447552447552\n",
      "Model 2 74.82517482517483\n",
      "Model 3 70.62937062937063\n",
      "Model 1 85.3146853146853\n",
      "Model 2 72.02797202797203\n",
      "Model 3 72.72727272727273\n",
      "Model 1 78.32167832167832\n",
      "Model 2 74.82517482517483\n",
      "Model 3 78.32167832167832\n",
      "Model 1 76.22377622377621\n",
      "Model 2 76.92307692307693\n",
      "Model 3 69.23076923076923\n",
      "Model 1 82.51748251748252\n",
      "Model 2 75.52447552447552\n",
      "Model 3 74.12587412587412\n",
      "Model 1 73.42657342657343\n",
      "Model 2 75.52447552447552\n",
      "Model 3 67.83216783216784\n",
      "Model 1 83.91608391608392\n",
      "Model 2 76.22377622377621\n",
      "Model 3 68.53146853146853\n",
      "Model 1 82.51748251748252\n",
      "Model 2 76.92307692307693\n",
      "Model 3 79.02097902097903\n",
      "Model 1 83.21678321678321\n",
      "Model 2 77.62237762237763\n",
      "Model 3 77.62237762237763\n",
      "Model 1 83.21678321678321\n",
      "Model 2 79.02097902097903\n",
      "Model 3 74.12587412587412\n",
      "Model 1 78.32167832167832\n",
      "Model 2 77.62237762237763\n",
      "Model 3 70.62937062937063\n",
      "Model 1 80.41958041958041\n",
      "Model 2 78.32167832167832\n",
      "Model 3 71.32867132867133\n",
      "Model 1 82.51748251748252\n",
      "Model 2 78.32167832167832\n",
      "Model 3 71.32867132867133\n",
      "Model 1 78.32167832167832\n",
      "Model 2 79.02097902097903\n",
      "Model 3 75.52447552447552\n",
      "Model 1 84.61538461538461\n",
      "Model 2 79.02097902097903\n",
      "Model 3 73.42657342657343\n",
      "Model 1 81.81818181818183\n",
      "Model 2 79.02097902097903\n",
      "Model 3 74.12587412587412\n",
      "Model 1 85.3146853146853\n",
      "Model 2 79.02097902097903\n",
      "Model 3 72.72727272727273\n",
      "Model 1 83.21678321678321\n",
      "Model 2 79.72027972027972\n",
      "Model 3 67.83216783216784\n",
      "Model 1 76.92307692307693\n",
      "Model 2 79.72027972027972\n",
      "Model 3 69.93006993006993\n",
      "Model 1 76.92307692307693\n",
      "Model 2 79.02097902097903\n",
      "Model 3 69.93006993006993\n",
      "Model 1 80.41958041958041\n",
      "Model 2 79.02097902097903\n",
      "Model 3 70.62937062937063\n",
      "Model 1 81.81818181818183\n",
      "Model 2 74.82517482517483\n",
      "Model 3 69.23076923076923\n",
      "Model 1 81.11888111888112\n",
      "Model 2 78.32167832167832\n",
      "Model 3 76.92307692307693\n",
      "Model 1 83.91608391608392\n",
      "Model 2 79.02097902097903\n",
      "Model 3 74.82517482517483\n",
      "Model 1 82.51748251748252\n",
      "Model 2 79.72027972027972\n",
      "Model 3 74.82517482517483\n",
      "Model 1 85.3146853146853\n",
      "Model 2 79.02097902097903\n",
      "Model 3 72.02797202797203\n",
      "Model 1 85.3146853146853\n",
      "Model 2 80.41958041958041\n",
      "Model 3 67.13286713286713\n",
      "Model 1 85.3146853146853\n",
      "Model 2 80.41958041958041\n",
      "Model 3 66.43356643356644\n",
      "Model 1 79.02097902097903\n",
      "Model 2 79.72027972027972\n",
      "Model 3 73.42657342657343\n",
      "Model 1 81.81818181818183\n",
      "Model 2 79.72027972027972\n",
      "Model 3 76.92307692307693\n",
      "Model 1 83.91608391608392\n",
      "Model 2 79.02097902097903\n",
      "Model 3 76.22377622377621\n",
      "Model 1 83.21678321678321\n",
      "Model 2 79.02097902097903\n",
      "Model 3 74.82517482517483\n",
      "Model 1 85.3146853146853\n",
      "Model 2 79.72027972027972\n",
      "Model 3 72.72727272727273\n",
      "Model 1 83.21678321678321\n",
      "Model 2 79.72027972027972\n",
      "Model 3 67.13286713286713\n",
      "Model 1 85.3146853146853\n",
      "Model 2 80.41958041958041\n",
      "Model 3 65.73426573426573\n",
      "Model 1 80.41958041958041\n",
      "Model 2 79.02097902097903\n",
      "Model 3 70.62937062937063\n",
      "Model 1 83.91608391608392\n",
      "Model 2 80.41958041958041\n",
      "Model 3 65.73426573426573\n",
      "Model 1 83.91608391608392\n",
      "Model 2 80.41958041958041\n",
      "Model 3 68.53146853146853\n",
      "Model 1 83.91608391608392\n",
      "Model 2 79.72027972027972\n",
      "Model 3 66.43356643356644\n",
      "Model 1 83.21678321678321\n",
      "Model 2 79.72027972027972\n",
      "Model 3 73.42657342657343\n",
      "Final 1 (13, 20) (20, 3)\n",
      "Final 2 (13, 20) (20, 3)\n"
     ]
    }
   ],
   "source": [
    "def ModelHybridNetwork(xVals, yVals):\n",
    "\n",
    "    inputLayerSize = xVals.shape[1]\n",
    "    hiddenLayerSize = 20\n",
    "    outputLayerSize = 3\n",
    "\n",
    "    learningRates = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    numEs = 1000\n",
    "\n",
    "    np.random.seed(20)\n",
    "\n",
    "    w1m1 = np.random.rand(inputLayerSize, hiddenLayerSize)\n",
    "    w2m1 = np.random.rand(hiddenLayerSize, outputLayerSize)\n",
    "\n",
    "    np.random.seed(20)\n",
    "\n",
    "    w1m2 = np.random.rand(inputLayerSize, hiddenLayerSize)\n",
    "    w2m2 = np.random.rand(hiddenLayerSize, outputLayerSize)\n",
    "\n",
    "    np.random.seed(20)\n",
    "\n",
    "    hybridW1 = np.random.rand(inputLayerSize, hiddenLayerSize)\n",
    "    hybridW2 = np.random.rand(hiddenLayerSize, outputLayerSize)\n",
    "\n",
    "    rateIndex1 = 0\n",
    "    rateIndex2 = 0\n",
    "\n",
    "    m1m1, m2m1 = Model1(xVals, yVals, w1m1, w2m1, learningRates[0])\n",
    "    prev_m1m2, prev_m2m2 = Model2(xVals, yVals, w1m2, w2m2, learningRates[rateIndex2], learningRates[rateIndex2])\n",
    "    \n",
    "    hybridW1 += (m1m1+prev_m1m2)*0.5\n",
    "    hybridW2 += (m2m1+prev_m2m2)*0.5\n",
    "\n",
    "    print(\"Initial 1\", m1m1.shape, m2m1.shape)\n",
    "    print(\"Initial 2\", prev_m1m2.shape, prev_m2m2.shape)\n",
    "    \n",
    "    model1HitRates = []\n",
    "    model2HitRates = []\n",
    "    model3HitRates = []\n",
    "\n",
    "    for i in range(1, 100):\n",
    "\n",
    "        m1m1, m2m1 = Model1(xVals, yVals, w1m1, w2m1, learningRates[0])\n",
    "\n",
    "        m1m2, m2m2 = Model2(xVals, yVals, w1m2, w2m2, learningRates[rateIndex1], learningRates[rateIndex2])\n",
    "\n",
    "        hybridW1 += (m1m1 + m1m2)*0.5\n",
    "        hybridW2 += (m2m1 + m2m2)*0.5\n",
    "\n",
    "        rateIndex1, rateIndex2 = getLearningRateIndex(prev_m1m2, prev_m2m2, m1m2, m2m2, rateIndex1, rateIndex2)\n",
    "\n",
    "        #print(\"Model 1\", evaluate(testPV_normalized, testDependentVariable, w1m1, w2m1))\n",
    "        model1HitRates.append(evaluate(testPV_normalized, testDependentVariable, w1m1, w2m1))\n",
    "        #print(\"Model 2\", evaluate(testPV_normalized, testDependentVariable, w1m2, w2m2))\n",
    "        model2HitRates.append(evaluate(testPV_normalized, testDependentVariable, w1m2, w2m2))\n",
    "        #print(\"Model 3\", evaluate(testPV_normalized, testDependentVariable, hybridW1, hybridW2))\n",
    "        model3HitRates.append( evaluate(testPV_normalized, testDependentVariable, hybridW1, hybridW2))\n",
    "\n",
    "        prev_m1m2 = m1m2\n",
    "        prev_m2m2 = m2m2\n",
    "\n",
    "    print(\"Final 1\", m1m2.shape, m2m1.shape)\n",
    "    print(\"Final 2\", prev_m1m2.shape, prev_m2m2.shape)\n",
    "\n",
    "ModelHybridNetwork(pV_normalized, transform(dependentVariable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndEvaluate(hiddenLayerSize, learningRate, numEpochs):\n",
    "    \n",
    "    inputSize = pV_normalized.shape[1]\n",
    "\n",
    "    outputLayerSize = 3\n",
    "\n",
    "    np.random.seed(12)\n",
    "\n",
    "    weightsInput_Hidden = np.random.rand(inputSize, hiddenLayerSize)\n",
    "    weightsHidden_Output = np.random.rand(hiddenLayerSize, outputLayerSize)\n",
    "\n",
    "    transformedDV = transform(dependentVariable)\n",
    "\n",
    "    weightsInput_Hidden, weightsHidden_Output = ModelNetwork1(pV_normalized, transformedDV, weightsInput_Hidden, weightsHidden_Output, learningRate, numEpochs)\n",
    "    \n",
    "    return evaluate(testPV_normalized, testDependentVariable, weightsInput_Hidden, weightsHidden_Output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGridSearch1():\n",
    "\n",
    "    hiddenLayerSizes = [4, 8, 12, 16, 20, 24, 28, 32, 36, 40]\n",
    "    learningRateSizes = [0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    maxV = 0\n",
    "    maxHLayer = 0\n",
    "    maxLRSize = 0\n",
    "\n",
    "    for hiddenLayerSize in hiddenLayerSizes:\n",
    "        tempResults = []\n",
    "        for learningRateSize in learningRateSizes:\n",
    "            val = trainAndEvaluate(hiddenLayerSize, learningRateSize, 10000)\n",
    "            maxV = max(maxV, val)\n",
    "\n",
    "            if (maxV == val):\n",
    "                maxHLayer = hiddenLayerSize\n",
    "                maxLRSize = learningRateSize\n",
    "\n",
    "            tempResults.append(val)\n",
    "        results.append(tempResults)\n",
    "    \n",
    "    return (np.array(results), maxHLayer, maxLRSize, maxV)\n",
    "\n",
    "#hlSizeVSlrSizeData = runGridSearch1()[0]\n",
    "\n",
    "#xLabels = [\"0.005\", \"0.01\", \"0.05\", \"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\"]\n",
    "#yLabels = [\"4\", \"8\", \"12\", \"16\", \"20\", \"24\", \"28\", \"32\", \"36\", \"40\"]\n",
    "\n",
    "#sns.heatmap(hlSizeVSlrSizeData, annot = True, cmap = \"coolwarm\", xticklabels = xLabels, yticklabels = yLabels)\n",
    "#plt.title(\"Hidden Layer Size vs Learning Rate Size Heatmap\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGridSearch2():\n",
    "    \n",
    "    hiddenLayerSizes = [4, 8, 12, 16, 20, 24, 28, 32, 36, 40]\n",
    "    numEpochs = [10, 100, 1000, 10000, 100000]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    maxV = 0\n",
    "    maxHLayer = 0\n",
    "    maxNumEs = 0\n",
    "\n",
    "\n",
    "    for hiddenLayerSize in hiddenLayerSizes:\n",
    "        tempResults = []\n",
    "        for numEs in numEpochs:\n",
    "            val = trainAndEvaluate(hiddenLayerSize, 0.1, numEs)\n",
    "            maxV = max(maxV, val)\n",
    "\n",
    "            if (maxV == val):\n",
    "                maxHLayer = hiddenLayerSize\n",
    "                maxNumEs = numEs\n",
    "            \n",
    "            tempResults.append(val)\n",
    "        results.append(tempResults)\n",
    "\n",
    "    return (np.array(results), maxHLayer, maxNumEs, maxV)\n",
    "\n",
    "#hlSizeVSnumEsData = runGridSearch2()[0]\n",
    "\n",
    "#xLabels = [\"10\", \"100\", \"1000\", \"10000\", \"100000\"]\n",
    "#yLabels = [\"4\", \"8\", \"12\", \"16\", \"20\", \"24\", \"28\", \"32\", \"36\", \"40\"]\n",
    "\n",
    "#sns.heatmap(hlSizeVSnumEsData, annot = True, cmap = \"coolwarm\", xticklabels = xLabels, yticklabels = yLabels)\n",
    "#plt.title(\"Hidden Layer Size vs Number of Epochs Heatmap\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGridSearch3():\n",
    "    \n",
    "    learningRateSizes = [0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    numEpochs = [10, 100, 1000, 10000, 100000]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    maxV = 0\n",
    "    maxLRSize = 0\n",
    "    maxNumEs = 0\n",
    "\n",
    "\n",
    "    for learningRateSize in learningRateSizes:\n",
    "        tempResults = []\n",
    "        for numEs in numEpochs:\n",
    "            val = trainAndEvaluate(20, learningRateSize, numEs)\n",
    "            maxV = max(maxV, val)\n",
    "\n",
    "            if (maxV == val):\n",
    "                maxLRSize = learningRateSize\n",
    "                maxNumEs = numEs\n",
    "            \n",
    "            tempResults.append(val)\n",
    "        results.append(tempResults)\n",
    "\n",
    "    return (np.array(results), maxLRSize, maxNumEs, maxV)\n",
    "\n",
    "#lrSizeVSnumEsData = runGridSearch3()[0]\n",
    "\n",
    "#xLabels = [\"10\", \"100\", \"1000\", \"10000\", \"100000\"]\n",
    "#yLabels = [\"0.005\", \"0.01\", \"0.05\", \"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\"]\n",
    "\n",
    "#sns.heatmap(lrSizeVSnumEsData, annot = True, cmap = \"coolwarm\", xticklabels = xLabels,yticklabels = yLabels)\n",
    "#plt.title(\"Learning Rate Size vs Number of Epochs Heatmap\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGridSearchAll():\n",
    "\n",
    "    hiddenLayerSizes = [4, 8, 12, 16, 20, 24, 28, 32, 36, 40]\n",
    "    learningRateSizes = [0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    numEpochs = [10, 100, 1000, 10000, 100000]\n",
    "\n",
    "    maxV = 0\n",
    "    maxHLSize = 0\n",
    "    maxLRSize = 0\n",
    "    maxNumEs = 0\n",
    "\n",
    "    for hiddenLayerSize in hiddenLayerSizes[::-1]:\n",
    "        for learningRateSize in learningRateSizes:\n",
    "            for numEs in numEpochs[::-1]:\n",
    "                val = trainAndEvaluate(hiddenLayerSize, learningRateSize, numEs)\n",
    "                maxV = max(maxV, val)\n",
    "\n",
    "                if (maxV == val):\n",
    "                    maxHLSize = hiddenLayerSize\n",
    "                    maxLRSize = learningRateSize\n",
    "                    maxNumEs = numEs\n",
    "\n",
    "    return (maxHLSize, maxLRSize, maxNumEs, maxV)\n",
    "\n",
    "#hlSize, lrSize, numEs, maxV = runGridSearchAll()\n",
    "\n",
    "#print(\"The best combination of hidden layer size, learning rate and number of epochs is\", hlSize, lrSize, numEs, \"yielding a hit rate of\", maxV, end = \".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
